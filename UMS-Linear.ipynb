{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyM+wZLJRLQX+/mBb2s4o5HN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/seyf1elislam/UMS-Linear/blob/main/UMS-Linear.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Univariate Multiscale Decomposition: Improving Linear Models for Univariate Long-term Time Series Forecasting (Pais'24 2024)\n",
        "## Running UMS-Linear Experiments\n",
        "\n",
        "In this notebook, we'll go through the steps required to run experiments with the UMS-Linear model, a state-of-the-art approach for univariate long-term time series forecasting. We'll cover the following:\n",
        "\n",
        "1. Downloading the official UMS-Linear repository from GitHub\n",
        "2. Obtaining benchmark datasets for time series forecasting tasks\n",
        "3. Defining the necessary arguments and configurations\n",
        "4. Importing the training functions from the downloaded code\n",
        "5. Running example experiments to train the UMS-Linear model\n",
        "\n",
        "The UMS-Linear model, introduced in the paper \"[Univariate Multiscale Decomposition: Improving Linear Models for Univariate Long-term Time Series Forecasting\" (Pais'24 2024)](https://github.com/seyf1elislam/UMS-Linear), combines multiscale decomposition and timestamp embedding with linear models to achieve impressive performance on various univariate time series forecasting benchmarks.\n",
        "\n",
        "By following the steps in this notebook, you'll gain practical experience in setting up and executing experiments with the UMS-Linear model using its official PyTorch implementation. This will provide you with a solid foundation for further exploration and application of the model to your own time series forecasting tasks.\n"
      ],
      "metadata": {
        "id": "yeedjwqaJIhh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Download data & define required functions**"
      ],
      "metadata": {
        "id": "ER6N-S61q2d4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N0IdOPOpuNxq",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title # **1) download repository and data**\n",
        "import os\n",
        "\n",
        "!git clone \"https://{github_token}@github.com/seyf1elislam/Forcasting_Memoire_M2.git\"\n",
        "!git clone \"https://github.com/seyf1elislam/UMS-Linear\"\n",
        "base_dir = 'UMS-Linear/'\n",
        "%cd /content/$base_dir\n",
        "!mkdir dataset\n",
        "#download data\n",
        "# from drive======================================\n",
        "# !pip install -U gdown==4.6.3\n",
        "# !gdown --folder 1KljxNkMfo5spW05K1QlIyk5ZLQlKfOxm  --output /content/$base_dir/dataset\n",
        "# !gdown --folder 1e4qvmCW_FJ0XSKCSyn0LA0fHzRpyG8iF  --output /content/$base_dir/dataset\n",
        "#from github==================================\n",
        "!wget \"https://github.com/hunter123520/Forecasting-Datasets/raw/main/datasets.rar\"\n",
        "!unrar x /content/$base_dir/datasets.rar /content/$base_dir/dataset/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title # **2) Define Arguments**\n",
        "import os\n",
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "%matplotlib inline\n",
        "\n",
        "from models import Informer, Autoformer, Transformer, Linear, NLinear,DLinear,UMSLinear\n",
        "global_model_dict = {\n",
        "            'Autoformer': Autoformer,\n",
        "            'Transformer': Transformer,\n",
        "            'Informer': Informer,\n",
        "            'DLinear': DLinear,\n",
        "            'NLinear': NLinear,\n",
        "            'Linear': Linear,\n",
        "            \"UMSmodel\":UMSLinear,\n",
        "        }\n",
        "\n",
        "\n",
        "class AttributeDict(dict):\n",
        "    __getattr__ = dict.__getitem__\n",
        "    __setattr__ = dict.__setitem__\n",
        "    __delattr__ = dict.__delitem__\n",
        "\n",
        "def getModelDict(model_class):\n",
        "  return AttributeDict({\"Model\":model_class})\n",
        "\n",
        "\n",
        "args = {\n",
        "    # basic config\n",
        "    \"is_training\":1, #'status'\n",
        "    \"train_only\":False, #'perform training on full input dataset without validation and testing'\n",
        "    \"model_id\":\"test\", #'model id'\n",
        "    \"model\":\"Autoformer\", #'model name, options: [Autoformer, Informer, Transformer]'\n",
        "\n",
        "    # data loader\n",
        "    \"data\":\"ETTh1\", #'dataset type'\n",
        "    \"root_path\":\"./data/ETT/\", #'root path of the data file'\n",
        "    \"data_path\":\"ETTh1.csv\", #'data file'\n",
        "    \"features\":\"M\", #'forecasting task, options:[M, S, MS]; M:multivariate predict multivariate, S:univariate predict univariate, MS:multivariate predict univariate'\n",
        "    \"target\":\"OT\", #'target feature in S or MS task'\n",
        "    \"freq\":\"h\", #'freq for time features encoding, options:[s:secondly, t:minutely, h:hourly, d:daily, b:business days, w:weekly, m:monthly], you can also use more detailed freq like 15min or 3h'\n",
        "    \"checkpoints\":'./checkpoints/', #'location of model checkpoints'\n",
        "\n",
        "    # forecasting task\n",
        "    \"seq_len\":96, #'input sequence length'\n",
        "    \"label_len\":48, #'start token length'\n",
        "    \"pred_len\":96, #'prediction sequence length'\n",
        "\n",
        "    # Linear / DLinear / Nlinear /UMS linear .\n",
        "    \"individual\":False, #treat each variate(channel) individually'\n",
        "\n",
        "    # Formers\n",
        "    \"embed_type\":0, #'0: default 1: value embedding + temporal embedding + positional embedding 2: value embedding + temporal embedding 3: value embedding + positional embedding 4: value embedding'\n",
        "    \"enc_in\":7, # 'encoder input size', DLinear with --individual, use this hyperparameter as the number of channels\n",
        "    \"dec_in\":7, #'decoder input size'\n",
        "    \"c_out\":7, #'output size'\n",
        "    \"d_model\":512, #'dimension of model'\n",
        "    \"n_heads\":8, #'num of heads'\n",
        "    \"e_layers\":2, #'num of encoder layers'\n",
        "    \"d_layers\":1, #'num of decoder layers'\n",
        "    \"d_ff\":2048, #'dimension of fcn'\n",
        "    \"moving_avg\":25, #'window size of moving average'\n",
        "    \"factor\":1, #'attn factor'\n",
        "    \"distil\":True, #'whether to use distilling in encoder, using this argument means not using distilling'\n",
        "    \"dropout\":0.05, #'dropout'\n",
        "    \"embed\":\"timeF\", #'time features encoding, options:[timeF, fixed, learned]'\n",
        "    \"activation\":\"gelu\", #'activation'\n",
        "    \"output_attention\":False, #'whether to output attention in ecoder'\n",
        "    \"do_predict\":False, #'whether to predict unseen future data'\n",
        "\n",
        "    # optimization\n",
        "    \"num_workers\":10, #'data loader num workers'\n",
        "    \"itr\":1, #'experiments times'\n",
        "    \"train_epochs\":10, #'train epochs'\n",
        "    \"batch_size\":32, #'batch size of train input data'\n",
        "    \"patience\":3, #'early stopping patience'\n",
        "    \"learning_rate\":0.0001,#'optimizer learning rate'\n",
        "    \"des\":\"test\", #'exp description'\n",
        "    \"loss\":\"mse\", #'loss function'\n",
        "    \"lradj\":\"type1\", #'adjust learning rate'\n",
        "    \"use_amp\":False, #'use automatic mixed precision training'\n",
        "\n",
        "    # GPU\n",
        "    \"use_gpu\":True, #'use gpu'\n",
        "    \"gpu\":0, #'gpu'\n",
        "    \"use_multi_gpu\":False, #'use multiple gpus'\n",
        "    \"devices\":\"0,1,2,3\", #'device ids of multile gpus'\n",
        "    \"test_flop\":False #'See utils/tools for usage'\n",
        "}\n",
        "\n",
        "#===================================\n",
        "args = AttributeDict(args)\n",
        "\n",
        "args.use_gpu = True if torch.cuda.is_available() and args.use_gpu else False\n",
        "\n",
        "args.is_training=1\n",
        "args.root_path=\"dataset/\"\n",
        "args.data_path=\"ETTh1.csv\"\n",
        "args.model_id=\"ETTh1_336_24\"\n",
        "args.data=\"ETTh1\"\n",
        "args.seq_len=336\n",
        "args.enc_in=1\n",
        "args.des='Exp'\n",
        "args.itr = 1\n",
        "args.batch_size = 32\n",
        "args.features = \"S\"\n",
        "\n",
        "#=========================\n",
        "args.learning_rate = 0.005\n",
        "args.model=\"DLinear\"\n",
        "args.pred_len=96\n",
        "#=========================\n",
        "\n",
        "if args.use_gpu and args.use_multi_gpu:\n",
        "    args.dvices = args.devices.replace(' ', '')\n",
        "    device_ids = args.devices.split(',')\n",
        "    args.device_ids = [int(id_) for id_ in device_ids]\n",
        "    args.gpu = args.device_ids[0]\n",
        "\n",
        "print('Args in experiment:')\n",
        "print(args)\n",
        "\n",
        "# ================\n"
      ],
      "metadata": {
        "id": "DGl_3vY9fQZa",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title # **3) define loss functions**\n",
        "from torch import nn\n",
        "def default_loss(batch_y,outputs,criterion):\n",
        "  loss = criterion(outputs, batch_y)\n",
        "  return loss\n",
        "\n",
        "def diff_loss(batch_y,outputs,criterion):\n",
        "  diff_data = outputs[:,1:] - outputs[:,:-1]\n",
        "  diff_data1 = batch_y[:,1:] - batch_y[:,:-1]\n",
        "  cr = nn.L1Loss()\n",
        "  loss = criterion(outputs,batch_y) *(1/3) + cr(outputs,batch_y)*(1/3) + cr(diff_data,diff_data1)*(1/3)\n",
        "  return loss\n",
        "\n",
        "def halfmae_halfmse_loss(batch_y,outputs,criterion):\n",
        "    mae = nn.L1Loss()\n",
        "    mse = nn.MSELoss()\n",
        "    loss = mae(outputs,batch_y) *(1/2) + mse(outputs,batch_y)*(1/2)\n",
        "    return loss"
      ],
      "metadata": {
        "id": "Gqw2-3yzkmiJ",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Training Section**\n"
      ],
      "metadata": {
        "id": "Og5XVAoi2l61"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title **Train Section**\n",
        "#@markdown   `Note : the parameters are overriding the args variabls` <br/> `the rest parameters can be edited from the args dict`\n",
        "\n",
        "#@markdown    for multivar data u need to change the enc_in\n",
        "#@markdown  >`\"ETTh2\",enc_in =  7` <br/>\n",
        "#@markdown  >`\"exchange_rate\", enc_in = 8`  <br/>\n",
        "#@markdown  >`\"weather\", enc_in = 21 `<br/>\n",
        "#@markdown  >`\"electricity\"=> enc_in = 321`<br/>\n",
        "#@markdown  >`\"traffic\"=> enc_in = 862`<br/>\n",
        "#@markdown  >`\"national_illness\"=> enc_in = 7 `<br/>\n",
        "%matplotlib inline\n",
        "from Training_new.full_train import full_train\n",
        "# from IPython.utils import io\n",
        "# with io.capture_output() as output: #this just in case u cant to hide the printing and take only the result\n",
        "result  =  full_train(\n",
        "    args,global_model_dict,\n",
        "    # use_print=False,\n",
        "    # use_edited_exp=False,#set this false to switch between the original exp_main and true for exp_main1,\n",
        "    #ModelName=====================================\n",
        "    # model=\"NLinear\",#this uses the name from the global_model_dict defined above\n",
        "    model=\"UMSmodel\",\n",
        "    # model=\"DLinear\",\n",
        "    # model=\"Autoformer\",\n",
        "    # model=\"DLinear\", custom_model=DLinear_Core, #this adds custom model to the globalc model_dict\n",
        "    # model=\"UMSmodel\", custom_model=UMSLinear.Model,\n",
        "    #Conf=============================================================\n",
        "    pred_lens=[96],#default [96,192,336,720]\n",
        "    #Here u setup ur Hyperparameters==================================================\n",
        "    learning_rate = 0.0005  ,batch_size = 128,seq_len = 512,\n",
        "    # learning_rate = 0.001  ,batch_size = 128,seq_len = 512,\n",
        "    #===============\n",
        "    data=\"ETTh1\",# ETTh2 / ETTm1 / ETTm2\n",
        "    features = \"S\",#default \"S\" for univariate \"M\" for multi variate\n",
        "    # enc_in =  1,#in channels default=1\n",
        "    #=====================================================\n",
        "    # train_epochs=20,#default is 20 epoch\n",
        "    patience = 3,#early stoping patience\n",
        "    #this can define ur custom lose or use on of the func defined above (default_loss,diff_loss,halfmae_halfmse_loss)\n",
        "    custom_loss = diff_loss\n",
        "    # custom_loss = default_loss\n",
        "    # custom_loss = halfmae_halfmse_loss\n",
        "    # fixed_seed=2024,#default None means random\n",
        "    )\n",
        "result"
      ],
      "metadata": {
        "id": "fvPF8Axi2oML",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}